{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "haJUNjSB60Kh"
   },
   "source": [
    "# word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mk4-Hpe1CH16"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T11:08:04.396269Z",
     "iopub.status.busy": "2023-07-28T11:08:04.395997Z",
     "iopub.status.idle": "2023-07-28T11:08:06.753162Z",
     "shell.execute_reply": "2023-07-28T11:08:06.752057Z"
    },
    "id": "RutaI-Tpev3T",
    "ExecuteTime": {
     "end_time": "2023-09-10T09:54:22.436315800Z",
     "start_time": "2023-09-10T09:54:22.430310500Z"
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "import string\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T11:08:06.771378Z",
     "iopub.status.busy": "2023-07-28T11:08:06.771077Z",
     "iopub.status.idle": "2023-07-28T11:08:06.775396Z",
     "shell.execute_reply": "2023-07-28T11:08:06.774355Z"
    },
    "id": "XkJ5299Tek6B",
    "ExecuteTime": {
     "end_time": "2023-09-10T09:54:22.817953900Z",
     "start_time": "2023-09-10T09:54:22.812955700Z"
    }
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aj--8RFK6fgW"
   },
   "source": [
    "### Generate training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dy5hl4lQ0B2M"
   },
   "source": [
    "Compile all the steps described above into a function that can be called on a list of vectorized sentences obtained from any text dataset. Notice that the sampling table is built before sampling skip-gram word pairs. You will use this function in the later sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T11:08:10.540373Z",
     "iopub.status.busy": "2023-07-28T11:08:10.540161Z",
     "iopub.status.idle": "2023-07-28T11:08:10.546597Z",
     "shell.execute_reply": "2023-07-28T11:08:10.545955Z"
    },
    "id": "63INISDEX1Hu",
    "ExecuteTime": {
     "end_time": "2023-09-10T09:54:23.781622500Z",
     "start_time": "2023-09-10T09:54:23.732464Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generates skip-gram pairs with negative sampling for a list of sequences\n",
    "# (int-encoded sentences) based on window size, number of negative samples\n",
    "# and vocabulary size.\n",
    "def generate_training_data(sequences, window_size, vocab_size, seed):\n",
    "    # Elements of each training example are appended to these lists.\n",
    "    targets, contexts, train_labels = [], [], []\n",
    "\n",
    "    # Build the sampling table for `vocab_size` tokens.\n",
    "    sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "\n",
    "    # Iterate over all sequences (sentences) in the dataset.\n",
    "    for sequence in tqdm.tqdm(sequences):\n",
    "\n",
    "        # Generate skip-gram pairs for a sequence (sentence).\n",
    "        skip_grams, labels = tf.keras.preprocessing.sequence.skipgrams(\n",
    "            sequence,\n",
    "            seed=seed,\n",
    "            vocabulary_size=vocab_size,\n",
    "            sampling_table=sampling_table,\n",
    "            window_size=window_size,\n",
    "            negative_samples=0.7)\n",
    "\n",
    "\n",
    "        for (target_word, context_word), label in zip(skip_grams, labels):\n",
    "            # Append each element from the training example to global lists.\n",
    "            targets.append(target_word)\n",
    "            contexts.append(context_word)\n",
    "            train_labels.append(label)\n",
    "\n",
    "    return targets, contexts, train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "shvPC8Ji2cMK"
   },
   "source": [
    "## Prepare training data for word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j5mbZsZu6uKg"
   },
   "source": [
    "With an understanding of how to work with one sentence for a skip-gram negative sampling based word2vec model, you can proceed to generate training examples from a larger list of sentences!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFlikI6L26nh"
   },
   "source": [
    "### Download text corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rEFavOgN98al"
   },
   "source": [
    "You will use a text file of Shakespeare's writing for this tutorial. Change the following line to run this code on your own data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTNZYqUs5C2V"
   },
   "source": [
    "Use the non empty lines to construct a `tf.data.TextLineDataset` object for the next steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T11:08:10.692071Z",
     "iopub.status.busy": "2023-07-28T11:08:10.691539Z",
     "iopub.status.idle": "2023-07-28T11:08:10.835087Z",
     "shell.execute_reply": "2023-07-28T11:08:10.834440Z"
    },
    "id": "ViDrwy-HjAs9",
    "ExecuteTime": {
     "end_time": "2023-09-10T09:54:27.139801100Z",
     "start_time": "2023-09-10T09:54:27.114355200Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-10 12:54:25.876120: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-10 12:54:26.118165: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-10 12:54:26.118214: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-10 12:54:26.121836: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-10 12:54:26.121880: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-10 12:54:26.121905: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-10 12:54:27.039357: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-10 12:54:27.039860: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-10 12:54:27.039869: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1726] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-09-10 12:54:27.039903: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-10 12:54:27.039929: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5385 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Ti, pci bus id: 0000:2d:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "BASE = \"/mnt/c/Users/mnusr/PycharmProjects/news_crawler/mlm_train_dataset\"\n",
    "text_ds = tf.data.TextLineDataset(\n",
    "    [f\"{BASE}/train_sentences_5.txt\", f\"{BASE}/train_sentences_4.txt\", f\"{BASE}/train_sentences_3.txt\",\n",
    "     f\"{BASE}/train_sentences_2.txt\",\n",
    "     f\"{BASE}/train_sentences_1.txt\"])  #.filter(lambda x: tf.cast(tf.strings.length(x), bool))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfsc88zE9upk"
   },
   "source": [
    "### Vectorize sentences from the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XfgZo8zR94KK"
   },
   "source": [
    "You can use the `TextVectorization` layer to vectorize sentences from the corpus. Learn more about using this layer in this [Text classification](https://www.tensorflow.org/tutorials/keras/text_classification) tutorial. Notice from the first few sentences above that the text needs to be in one case and punctuation needs to be removed. To do this, define a `custom_standardization function` that can be used in the TextVectorization layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T11:08:10.839001Z",
     "iopub.status.busy": "2023-07-28T11:08:10.838735Z",
     "iopub.status.idle": "2023-07-28T11:08:10.854250Z",
     "shell.execute_reply": "2023-07-28T11:08:10.853659Z"
    },
    "id": "2MlsXzo-ZlfK",
    "ExecuteTime": {
     "end_time": "2023-09-10T09:54:27.231644200Z",
     "start_time": "2023-09-10T09:54:27.140800900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now, create a custom standardization function to lowercase the text and\n",
    "# remove punctuation.\n",
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    return tf.strings.regex_replace(lowercase,\n",
    "                                    '[%s]' % re.escape(string.punctuation), '')\n",
    "\n",
    "\n",
    "# Define the vocabulary size and the number of words in a sequence.\n",
    "vocab_size = 40000\n",
    "sequence_length = 15\n",
    "\n",
    "# Use the `TextVectorization` layer to normalize, split, and map strings to\n",
    "# integers. Set the `output_sequence_length` length to pad all samples to the\n",
    "# same length.\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g92LuvnyBmz1"
   },
   "source": [
    "Call `TextVectorization.adapt` on the text dataset to create vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T11:08:10.857314Z",
     "iopub.status.busy": "2023-07-28T11:08:10.857086Z",
     "iopub.status.idle": "2023-07-28T11:08:12.087865Z",
     "shell.execute_reply": "2023-07-28T11:08:12.087059Z"
    },
    "id": "seZau_iYMPFT",
    "ExecuteTime": {
     "end_time": "2023-09-10T09:55:31.472827500Z",
     "start_time": "2023-09-10T09:54:29.458914800Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-10 12:55:26.569982: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 9406767263800707434\n",
      "2023-09-10 12:55:26.570033: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 2575501437028159367\n",
      "2023-09-10 12:55:26.570055: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 13370593030879677044\n"
     ]
    }
   ],
   "source": [
    "vectorize_layer.adapt(text_ds.batch(4096 * 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jg2z7eeHMnH-"
   },
   "source": [
    "Once the state of the layer has been adapted to represent the text corpus, the vocabulary can be accessed with `TextVectorization.get_vocabulary`. This function returns a list of all vocabulary tokens sorted (descending) by their frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T11:08:12.092193Z",
     "iopub.status.busy": "2023-07-28T11:08:12.091578Z",
     "iopub.status.idle": "2023-07-28T11:08:12.102367Z",
     "shell.execute_reply": "2023-07-28T11:08:12.101770Z"
    },
    "id": "jgw9pTA7MRaU",
    "ExecuteTime": {
     "end_time": "2023-09-10T09:55:31.484854500Z",
     "start_time": "2023-09-10T09:55:31.474830Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 've', 'bir', 'bu', 'da', 'için', 'de', 'ile', 'olarak', 'çok', 'daha', 'olan', 'ise', 'en', 'sonra', 'kadar', 'göre', 'gibi', 'her', 'bin', 'yüzde', 'olduğunu', 'olduğu', 'yer', 'var', 'ilgili', 'son', 'yeni', 'tarafından', 'dedi', 'o', 'büyük', 'başkanı', 'devam', 'yapılan', 'ne', 'ilk', 'ardından', 'ama', 'yıl', 'diye', 'genel', 'önce', '2', 'oldu', 'türkiye', 'etti', 'ancak', '3']\n"
     ]
    }
   ],
   "source": [
    "# Save the created vocabulary for reference.\n",
    "inverse_vocab = vectorize_layer.get_vocabulary()\n",
    "print(inverse_vocab[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DOQ30Tx6KA2G"
   },
   "source": [
    "The `vectorize_layer` can now be used to generate vectors for each element in the `text_ds` (a `tf.data.Dataset`). Apply `Dataset.batch`, `Dataset.prefetch`, `Dataset.map`, and `Dataset.unbatch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T11:08:12.105508Z",
     "iopub.status.busy": "2023-07-28T11:08:12.105144Z",
     "iopub.status.idle": "2023-07-28T11:08:12.146664Z",
     "shell.execute_reply": "2023-07-28T11:08:12.145982Z"
    },
    "id": "yUVYrDp0araQ"
   },
   "outputs": [],
   "source": [
    "# Vectorize the data in text_ds.\n",
    "text_vector_ds = text_ds.batch(4096 * 8).prefetch(AUTOTUNE).map(vectorize_layer, num_parallel_calls=tf.data.AUTOTUNE).unbatch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7YyH_SYzB72p"
   },
   "source": [
    "### Obtain sequences from the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFUQLX0_KaRC"
   },
   "source": [
    "You now have a `tf.data.Dataset` of integer encoded sentences. To prepare the dataset for training a word2vec model, flatten the dataset into a list of sentence vector sequences. This step is required as you would iterate over each sentence in the dataset to produce positive and negative examples.\n",
    "\n",
    "Note: Since the `generate_training_data()` defined earlier uses non-TensorFlow Python/NumPy functions, you could also use a `tf.py_function` or `tf.numpy_function` with `tf.data.Dataset.map`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sequences = list(text_vector_ds.as_numpy_iterator())\n",
    "print(len(sequences))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Inspect a few examples from `sequences`:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T11:08:16.336201Z",
     "iopub.status.busy": "2023-07-28T11:08:16.335630Z",
     "iopub.status.idle": "2023-07-28T11:08:16.340222Z",
     "shell.execute_reply": "2023-07-28T11:08:16.339581Z"
    },
    "id": "WZf1RIbB2Dfb"
   },
   "outputs": [],
   "source": [
    "for seq in sequences[:5]:\n",
    "    print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDzSOjNwCWNh"
   },
   "source": [
    "### Generate training examples from sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BehvYr-nEKyY"
   },
   "source": [
    "`sequences` is now a list of int encoded sentences. Just call the `generate_training_data` function defined earlier to generate training examples for the word2vec model. To recap, the function iterates over each word from each sequence to collect positive and negative context words. Length of target, contexts and labels should be the same, representing the total number of training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T11:08:16.343315Z",
     "iopub.status.busy": "2023-07-28T11:08:16.343060Z",
     "iopub.status.idle": "2023-07-28T11:09:02.403710Z",
     "shell.execute_reply": "2023-07-28T11:09:02.402881Z"
    },
    "id": "44DJ22M6nX5o"
   },
   "outputs": [],
   "source": [
    "targets, contexts, labels = generate_training_data(\n",
    "    sequences=sequences,\n",
    "    window_size=2,\n",
    "    num_ns=4,\n",
    "    vocab_size=vocab_size,\n",
    "    seed=SEED)\n",
    "\n",
    "targets = np.array(targets)\n",
    "contexts = np.array(contexts)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print('\\n')\n",
    "print(f\"targets.shape: {targets.shape}\")\n",
    "print(f\"contexts.shape: {contexts.shape}\")\n",
    "print(f\"labels.shape: {labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "del sequences\n",
    "import gc\n",
    "\n",
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97PqsusOFEpc"
   },
   "source": [
    "### Configure the dataset for performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7jnFVySViQTj"
   },
   "source": [
    "To perform efficient batching for the potentially large number of training examples, use the `tf.data.Dataset` API. After this step, you would have a `tf.data.Dataset` object of `(target_word, context_word), (label)` elements to train your word2vec model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T11:09:02.407497Z",
     "iopub.status.busy": "2023-07-28T11:09:02.407233Z",
     "iopub.status.idle": "2023-07-28T11:09:02.424122Z",
     "shell.execute_reply": "2023-07-28T11:09:02.423384Z"
    },
    "id": "nbu8PxPSnVY2"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1024\n",
    "BUFFER_SIZE = 5000\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((targets[:50_000_000], contexts[:50_000_000]), labels[:50_000_000]))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).prefetch(AUTOTUNE)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1S-CmUMszyEf"
   },
   "source": [
    "## Model and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQFqaBMPwBqC"
   },
   "source": [
    "The word2vec model can be implemented as a classifier to distinguish between true context words from skip-grams and false context words obtained through negative sampling. You can perform a dot product multiplication between the embeddings of target and context words to obtain predictions for labels and compute the loss function against true labels in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oc7kTbiwD9sy"
   },
   "source": [
    "### Subclassed word2vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jvr9pM1G1sQN"
   },
   "source": [
    "Use the [Keras Subclassing API](https://www.tensorflow.org/guide/keras/custom_layers_and_models) to define your word2vec model with the following layers:\n",
    "\n",
    "* `target_embedding`: A `tf.keras.layers.Embedding` layer, which looks up the embedding of a word when it appears as a target word. The number of parameters in this layer are `(vocab_size * embedding_dim)`.\n",
    "* `context_embedding`: Another `tf.keras.layers.Embedding` layer, which looks up the embedding of a word when it appears as a context word. The number of parameters in this layer are the same as those in `target_embedding`, i.e. `(vocab_size * embedding_dim)`.\n",
    "* `dots`: A `tf.keras.layers.Dot` layer that computes the dot product of target and context embeddings from a training pair.\n",
    "* `flatten`: A `tf.keras.layers.Flatten` layer to flatten the results of `dots` layer into logits.\n",
    "\n",
    "With the subclassed model, you can define the `call()` function that accepts `(target, context)` pairs which can then be passed into their corresponding embedding layer. Reshape the `context_embedding` to perform a dot product with `target_embedding` and return the flattened result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KiAwuIqqw7-7"
   },
   "source": [
    "Key point: The `target_embedding` and `context_embedding` layers can be shared as well. You could also use a concatenation of both embeddings as the final word2vec embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T11:09:02.437574Z",
     "iopub.status.busy": "2023-07-28T11:09:02.437361Z",
     "iopub.status.idle": "2023-07-28T11:09:02.442674Z",
     "shell.execute_reply": "2023-07-28T11:09:02.442081Z"
    },
    "id": "i9ec-sS6xd8Z"
   },
   "outputs": [],
   "source": [
    "class Word2Vec(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.target_embedding = layers.Embedding(vocab_size,\n",
    "                                                 embedding_dim,\n",
    "                                                 input_length=1,\n",
    "                                                 name=\"w2v_embedding\")\n",
    "        self.context_embedding = layers.Embedding(vocab_size,\n",
    "                                                  embedding_dim,\n",
    "                                                  input_length=1)\n",
    "\n",
    "        self.dot = tf.keras.layers.Dot(axes=-1)\n",
    "\n",
    "    def call(self, inputs, training=False, mask=None):\n",
    "        target, context = inputs\n",
    "        # target: (batch, dummy?)  # The dummy axis doesn't exist in TF2.7+\n",
    "        # context: (batch, context)\n",
    "        if len(target.shape) == 2:\n",
    "            target = tf.squeeze(target, axis=1)\n",
    "        # target: (batch,)\n",
    "        word_emb = self.target_embedding(target)\n",
    "        # word_emb: (batch, embed)\n",
    "        context_emb = self.context_embedding(context)\n",
    "        # context_emb: (batch, context, embed)\n",
    "        dots = self.dot([word_emb, context_emb])\n",
    "        # dots: (batch, context)\n",
    "        return dots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-RLKz9LFECXu"
   },
   "source": [
    "### Define loss function and compile model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I3Md-9QanqBM"
   },
   "source": [
    "It's time to build your model! Instantiate your word2vec class with an embedding dimension of 128 (you could experiment with different values). Compile the model with the `tf.keras.optimizers.Adam` optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T11:09:02.445669Z",
     "iopub.status.busy": "2023-07-28T11:09:02.445466Z",
     "iopub.status.idle": "2023-07-28T11:09:02.465259Z",
     "shell.execute_reply": "2023-07-28T11:09:02.464684Z"
    },
    "id": "ekQg_KbWnnmQ"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
    "word2vec.compile(optimizer='adam',\n",
    "                 loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                 run_eagerly=False,\n",
    "                 jit_compile=True,\n",
    "                 metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5wEBotlGZ7B"
   },
   "source": [
    "Train the model on the `dataset` for some number of epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T11:09:02.475254Z",
     "iopub.status.busy": "2023-07-28T11:09:02.475029Z",
     "iopub.status.idle": "2023-07-28T11:09:21.188906Z",
     "shell.execute_reply": "2023-07-28T11:09:21.188254Z"
    },
    "id": "gmC1BJalEZIY"
   },
   "outputs": [],
   "source": [
    "word2vec.fit(dataset, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TaDW2tIIz8fL"
   },
   "source": [
    "## Embedding lookup and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zp5rv01WG2YA"
   },
   "source": [
    "Obtain the weights from the model using `Model.get_layer` and `Layer.get_weights`. The `TextVectorization.get_vocabulary` function provides the vocabulary to build a metadata file with one token per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T11:09:21.192606Z",
     "iopub.status.busy": "2023-07-28T11:09:21.192346Z",
     "iopub.status.idle": "2023-07-28T11:09:21.204238Z",
     "shell.execute_reply": "2023-07-28T11:09:21.203589Z"
    },
    "id": "_Uamp1YH8RzU"
   },
   "outputs": [],
   "source": [
    "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gWzdmUzS8Sl4"
   },
   "source": [
    "Create and save the vectors and metadata files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T11:09:21.207489Z",
     "iopub.status.busy": "2023-07-28T11:09:21.207248Z",
     "iopub.status.idle": "2023-07-28T11:09:21.523165Z",
     "shell.execute_reply": "2023-07-28T11:09:21.522357Z"
    },
    "id": "VLIahl9s53XT"
   },
   "outputs": [],
   "source": [
    "out_v = io.open('vectors_tr_2.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata_tr_2.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "    if index == 0:\n",
    "        continue  # skip 0, it's padding.\n",
    "    vec = weights[index]\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "    out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "word2vec.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
