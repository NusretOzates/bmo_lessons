{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from datasets import load_dataset\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "def standardization(input_string):\n",
    "    text = tf.strings.lower(input_string)\n",
    "    # Keep space, a to z, and select punctuation.\n",
    "    text = tf.strings.regex_replace(text, '[^ a-z.?!,]', '')\n",
    "    # Add spaces around punctuation.\n",
    "    text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
    "    # Strip whitespace.\n",
    "    text = tf.strings.strip(text)\n",
    "\n",
    "    text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
    "\n",
    "    return text\n",
    "\n",
    "def process_text(inputs):\n",
    "    context = inputs[\"tr\"]\n",
    "    target = inputs[\"en\"]\n",
    "\n",
    "    context = tr_vectorizer(context).to_tensor()\n",
    "\n",
    "    target = en_vectorizer(target)\n",
    "    # This is what we will give to the RNN, from [START] until before [END]\n",
    "    targ_in = target[:, :-1].to_tensor()\n",
    "    # This is what we want from the RNN to output, from 1 (after [START]) until [END]\n",
    "    targ_out = target[:, 1:].to_tensor()\n",
    "    \n",
    "    return (context, targ_in), targ_out\n",
    "\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "train_dataset = load_dataset(\"opus100\", \"en-tr\", split=\"train[:500000]\")\n",
    "train_dataset = train_dataset.map(lambda x: {\"tr\": x[\"translation\"]['tr'], \"en\": x[\"translation\"]['en']})\n",
    "train_dataset = train_dataset.remove_columns([\"translation\"])\n",
    "train_dataset = train_dataset.to_tf_dataset(BATCH_SIZE, columns=[\"tr\", \"en\"])\n",
    "\n",
    "tr_vectorizer = layers.TextVectorization(max_tokens=30000, standardize=standardization, ragged=True)\n",
    "tr_vectorizer.adapt(train_dataset.map(lambda x: x[\"tr\"]))\n",
    "print(tr_vectorizer.get_vocabulary()[:10])\n",
    "\n",
    "en_vectorizer = layers.TextVectorization(max_tokens=30000, standardize=standardization, ragged=True)\n",
    "en_vectorizer.adapt(train_dataset.map(lambda x: x[\"en\"]))\n",
    "print(en_vectorizer.get_vocabulary()[:10])\n",
    "\n",
    "train_dataset = train_dataset.map(process_text, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "test_dataset = load_dataset(\"opus100\", \"en-tr\", split=\"test\")\n",
    "test_dataset = test_dataset.map(lambda x: {\"tr\": x[\"translation\"]['tr'], \"en\": x[\"translation\"]['en']})\n",
    "test_dataset = test_dataset.remove_columns([\"translation\"])\n",
    "test_dataset = test_dataset.to_tf_dataset(BATCH_SIZE, columns=[\"tr\", \"en\"])\n",
    "test_dataset = test_dataset.map(process_text, num_parallel_calls=tf.data.AUTOTUNE)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "12ed0afad642b305"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Encoder(layers.Layer):\n",
    "    def __init__(self, enc_units:int):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.gru = layers.Bidirectional(\n",
    "            merge_mode=\"sum\",\n",
    "            layer=layers.GRU(enc_units)\n",
    "        )\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        output = self.gru(inputs,mask=mask)\n",
    "        return output\n",
    "\n",
    "\n",
    "class Decoder(layers.Layer):\n",
    "    def __init__(self, vocab_size:int, dec_units:int):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.gru = layers.GRU(dec_units, return_sequences=True)\n",
    "        self.fc = layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        # X is the target sentence, y is the encoder output\n",
    "        x, state = inputs\n",
    "        output = self.gru(x, initial_state=state,mask=mask[0])\n",
    "        x = self.fc(output)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Seq2Seq(tf.keras.Model):\n",
    "    def __init__(self, encoder, decoder,enc_vocab_size,dec_vocab_size, embedding_dim):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.encoder_embedding = layers.Embedding(enc_vocab_size, embedding_dim, mask_zero=True)\n",
    "        self.decoder_embedding = layers.Embedding(dec_vocab_size, embedding_dim, mask_zero=True)\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        x, y = inputs\n",
    "        x = self.encoder_embedding(x)\n",
    "        y = self.decoder_embedding(y)\n",
    "        \n",
    "        state = self.encoder(x)\n",
    "        y = self.decoder((y, state))\n",
    "        return y\n",
    "\n",
    "\n",
    "def masked_loss(y_true, y_pred):\n",
    "    # Calculate the loss for each item in the batch.\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction='none')\n",
    "    loss = loss_fn(y_true, y_pred)\n",
    "\n",
    "    # Mask off the losses on padding.\n",
    "    mask = tf.cast(y_true != 0, loss.dtype)\n",
    "    loss *= mask\n",
    "\n",
    "    # Return the total.\n",
    "    return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "def masked_acc(y_true, y_pred):\n",
    "    # Calculate the loss for each item in the batch.\n",
    "    y_pred = tf.argmax(y_pred, axis=-1)\n",
    "    y_pred = tf.cast(y_pred, y_true.dtype)\n",
    "\n",
    "    match = tf.cast(y_true == y_pred, tf.float32)\n",
    "    mask = tf.cast(y_true != 0, tf.float32)\n",
    "\n",
    "    return tf.reduce_sum(match) / tf.reduce_sum(mask)\n",
    "\n",
    "DIM = 256\n",
    "enc = Encoder(enc_units=DIM)\n",
    "dec = Decoder(dec_units=DIM,vocab_size=en_vectorizer.vocabulary_size())\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "model = Seq2Seq(encoder=enc, decoder=dec,enc_vocab_size=tr_vectorizer.vocabulary_size(),dec_vocab_size=en_vectorizer.vocabulary_size(),embedding_dim=DIM)\n",
    "model.compile(optimizer=optimizer, loss=masked_loss, metrics=[masked_acc], run_eagerly=False)\n",
    "model.fit(train_dataset, validation_data=test_dataset, epochs=10)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.fit(train_dataset, validation_data=test_dataset, epochs=10)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b02fa5694e22950e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def translate(input_text, max_length=50):\n",
    "\n",
    "    input_text = tf.convert_to_tensor([input_text])\n",
    "    input_text = tr_vectorizer(input_text)\n",
    "    encoder_input = input_text.to_tensor()\n",
    "    decoder_input = tf.expand_dims([2], 0)\n",
    "\n",
    "    encoder = model.layers[0]\n",
    "    decoder = model.layers[1]\n",
    "\n",
    "    states = encoder(encoder_input)\n",
    "\n",
    "    end = False\n",
    "    results = []\n",
    "    while not end:\n",
    "        output, states = decoder([decoder_input, states])\n",
    "        output = tf.argmax(output, -1)\n",
    "        print(output)\n",
    "        results.append(output.numpy()[0, 0])\n",
    "\n",
    "        if output.numpy()[0][0] == 3 or len(results) >= max_length:\n",
    "            end = True\n",
    "        decoder_input = output\n",
    "\n",
    "    results =[en_vectorizer.get_vocabulary()[i] for i in results]\n",
    "    return \" \".join(results)\n",
    "\n",
    "print(translate(\"Öğrenci olmak istiyorum.\"))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4fdb4b9c5fb23ca6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"student\" in en_vectorizer.get_vocabulary()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e877b41dcf637c40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 5  # This should be at least 10 for convergence\n",
    "MAX_SEQUENCE_LENGTH = 40\n",
    "TR_VOCAB_SIZE = 15000\n",
    "EN_VOCAB_SIZE = 15000\n",
    "\n",
    "EMBED_DIM = 256\n",
    "INTERMEDIATE_DIM = 2048\n",
    "NUM_HEADS = 8\n",
    "\n",
    "reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "121d0dc4a73ec005"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
