{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-09-12T14:53:53.194654200Z",
     "start_time": "2023-09-12T14:53:28.499267400Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-12 17:53:28.672771: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-12 17:53:29.211298: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-09-12 17:53:32.291135: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-12 17:53:32.313838: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-12 17:53:32.313885: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-12 17:53:32.315590: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-12 17:53:32.315645: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-12 17:53:32.315678: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-12 17:53:32.907167: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-12 17:53:32.907230: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-12 17:53:32.907236: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1722] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-09-12 17:53:32.907266: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-12 17:53:32.907289: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5355 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Ti, pci bus id: 0000:2d:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', '[START]', '[END]', '.', ',', '?', 'bir', '!', 'bu']\n",
      "['', '[UNK]', '[START]', '[END]', '.', ',', 'you', 'the', '?', 'i']\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from datasets import load_dataset\n",
    "import tensorflow as tf\n",
    "\n",
    "def standardization(input_string):\n",
    "    text = tf.strings.lower(input_string)\n",
    "    # Keep space, a to z, and select punctuation.\n",
    "    text = tf.strings.regex_replace(text, '[^ a-z.?!,]', '')\n",
    "    # Add spaces around punctuation.\n",
    "    text = tf.strings.regex_replace(text, '[.?!,Â¿]', r' \\0 ')\n",
    "    # Strip whitespace.\n",
    "    text = tf.strings.strip(text)\n",
    "\n",
    "    text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
    "\n",
    "    return text\n",
    "\n",
    "def process_text(inputs):\n",
    "    context = inputs[\"tr\"]\n",
    "    target = inputs[\"en\"]\n",
    "\n",
    "    context = tr_vectorizer(context).to_tensor()\n",
    "\n",
    "    target = en_vectorizer(target)\n",
    "    targ_in = target[:, :-1].to_tensor()\n",
    "    targ_out = target[:, 1:].to_tensor()\n",
    "    return (context, targ_in), targ_out\n",
    "\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "train_dataset = load_dataset(\"opus100\", \"en-tr\", split=\"train[:500000]\")\n",
    "train_dataset = train_dataset.map(lambda x: {\"tr\": x[\"translation\"]['tr'], \"en\": x[\"translation\"]['en']})\n",
    "train_dataset = train_dataset.remove_columns([\"translation\"])\n",
    "train_dataset = train_dataset.to_tf_dataset(BATCH_SIZE, columns=[\"tr\", \"en\"])\n",
    "\n",
    "tr_vectorizer = layers.TextVectorization(max_tokens=10000, standardize=standardization, ragged=True)\n",
    "tr_vectorizer.adapt(train_dataset.map(lambda x: x[\"tr\"], num_parallel_calls=tf.data.AUTOTUNE))\n",
    "print(tr_vectorizer.get_vocabulary()[:10])\n",
    "\n",
    "en_vectorizer = layers.TextVectorization(max_tokens=10000, standardize=standardization, ragged=True)\n",
    "en_vectorizer.adapt(train_dataset.map(lambda x: x[\"en\"], num_parallel_calls=tf.data.AUTOTUNE))\n",
    "print(en_vectorizer.get_vocabulary()[:10])\n",
    "\n",
    "train_dataset = train_dataset.map(process_text, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "test_dataset = load_dataset(\"opus100\", \"en-tr\", split=\"test\")\n",
    "test_dataset = test_dataset.map(lambda x: {\"tr\": x[\"translation\"]['tr'], \"en\": x[\"translation\"]['en']})\n",
    "test_dataset = test_dataset.remove_columns([\"translation\"])\n",
    "test_dataset = test_dataset.to_tf_dataset(BATCH_SIZE, columns=[\"tr\", \"en\"])\n",
    "test_dataset = test_dataset.map(process_text, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class Encoder(layers.Layer):\n",
    "    def __init__(self, enc_units:int):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.gru = layers.Bidirectional(\n",
    "            merge_mode=\"sum\",\n",
    "            layer=layers.GRU(enc_units,return_sequences=True)\n",
    "        )\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        output = self.gru(inputs,mask=mask)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "class Attention(layers.Layer):\n",
    "\n",
    "    def __init__(self, units):\n",
    "        super(Attention, self).__init__()\n",
    "        self.mha = layers.MultiHeadAttention(num_heads=1, key_dim=units)\n",
    "        self.layernorm = layers.LayerNormalization()\n",
    "        self.add = layers.Add()\n",
    "\n",
    "    def call(self, inputs, *args, **kwargs):\n",
    "        x, context_sequences = inputs\n",
    "        attention_output = self.mha(query=x, value=context_sequences, key=context_sequences)\n",
    "        x = self.add([attention_output, x])\n",
    "        x = self.layernorm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(layers.Layer):\n",
    "    def __init__(self, vocab_size, dec_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.gru = layers.GRU(dec_units, return_sequences=True, return_state=True)\n",
    "        self.fc = layers.Dense(vocab_size)\n",
    "        self.attention = Attention(dec_units)\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None, state=None, return_state=False):\n",
    "        y, context_sequences = inputs\n",
    "        output, state = self.gru(y, initial_state=state,mask=mask)\n",
    "\n",
    "        output = self.attention([output, context_sequences])\n",
    "        x = self.fc(output)\n",
    "        if return_state:\n",
    "            return x, state\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "\n",
    "class Seq2Seq(tf.keras.Model):\n",
    "    def __init__(self, encoder, decoder,enc_vocab_size,dec_vocab_size, embedding_dim):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.encoder_embedding = layers.Embedding(enc_vocab_size, embedding_dim, mask_zero=True)\n",
    "        self.decoder_embedding = layers.Embedding(dec_vocab_size, embedding_dim, mask_zero=True)\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        x, y = inputs\n",
    "        x = self.encoder_embedding(x)\n",
    "        y = self.decoder_embedding(y)\n",
    "        \n",
    "        state = self.encoder(x)\n",
    "        y = self.decoder((y, state))\n",
    "        return y\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "def masked_loss(y_true, y_pred):\n",
    "    # Calculate the loss for each item in the batch.\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction='none')\n",
    "    loss = loss_fn(y_true, y_pred)\n",
    "    # Mask off the losses on padding.\n",
    "    mask = tf.cast(y_true != 0, loss.dtype)\n",
    "    loss *= mask\n",
    "\n",
    "    # Return the total.\n",
    "    return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "\n",
    "def masked_acc(y_true, y_pred):\n",
    "    # Calculate the loss for each item in the batch.\n",
    "    y_pred = tf.argmax(y_pred, axis=-1)\n",
    "    y_pred = tf.cast(y_pred, y_true.dtype)\n",
    "\n",
    "    match = tf.cast(y_true == y_pred, tf.float32)\n",
    "    mask = tf.cast(y_true != 0, tf.float32)\n",
    "\n",
    "    return tf.reduce_sum(match) / tf.reduce_sum(mask)\n",
    "\n",
    "DIM = 256\n",
    "enc = Encoder(enc_units=DIM)\n",
    "dec = Decoder(vocab_size=en_vectorizer.vocabulary_size(), dec_units=DIM)\n",
    "\n",
    "\n",
    "model = Seq2Seq(encoder=enc, decoder=dec,enc_vocab_size=tr_vectorizer.vocabulary_size(),dec_vocab_size=en_vectorizer.vocabulary_size(),embedding_dim=DIM)\n",
    "model.compile(optimizer=optimizer, loss=masked_loss, metrics=[masked_acc], run_eagerly=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-12T14:53:53.351268200Z",
     "start_time": "2023-09-12T14:53:53.201252300Z"
    }
   },
   "id": "112dbd125424d94a"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-12 17:53:53.361117: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_14' with dtype string\n",
      "\t [[{{node Placeholder/_14}}]]\n",
      "2023-09-12 17:53:53.361385: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_17' with dtype int64\n",
      "\t [[{{node Placeholder/_17}}]]\n",
      "2023-09-12 17:53:55.518685: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-09-12 17:53:57.927907: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-09-12 17:53:58.663697: W tensorflow/core/common_runtime/type_inference.cc:339] Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT: expected compatible input types, but input 1:\n",
      "type_id: TFT_OPTIONAL\n",
      "args {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_TENSOR\n",
      "    args {\n",
      "      type_id: TFT_INT32\n",
      "    }\n",
      "  }\n",
      "}\n",
      " is neither a subtype nor a supertype of the combined inputs preceding it:\n",
      "type_id: TFT_OPTIONAL\n",
      "args {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_TENSOR\n",
      "    args {\n",
      "      type_id: TFT_INT8\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\twhile inferring type of node 'cond_43/output/_23'\n",
      "2023-09-12 17:54:00.408286: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n",
      "2023-09-12 17:54:00.869534: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-09-12 17:54:01.174800: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7efcccfa2fc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-09-12 17:54:01.174836: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce RTX 3070 Ti, Compute Capability 8.6\n",
      "2023-09-12 17:54:01.204306: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-09-12 17:54:01.457348: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5990/7813 [======================>.......] - ETA: 1:14 - loss: 3.3125 - masked_acc: 0.4237"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_dataset, epochs=15, validation_data=test_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-12T14:58:09.283761100Z",
     "start_time": "2023-09-12T14:53:53.353267100Z"
    }
   },
   "id": "b02fa5694e22950e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def translate(input_text, max_length=50):\n",
    "\n",
    "    input_text = tf.convert_to_tensor([input_text])\n",
    "    input_text = tr_vectorizer(input_text)\n",
    "    encoder_input = input_text.to_tensor()\n",
    "    decoder_input = tf.expand_dims([2], 0)\n",
    "\n",
    "    encoder = model.layers[0]\n",
    "    decoder = model.layers[1]\n",
    "\n",
    "    encoder_states = encoder(encoder_input)\n",
    "    state = None\n",
    "\n",
    "    end = False\n",
    "    results = []\n",
    "    while not end:\n",
    "        output, state = decoder([decoder_input, encoder_states], return_state=True,state=state)\n",
    "        output = tf.argmax(output, -1)\n",
    "        print(output)\n",
    "        results.append(output.numpy()[0, 0])\n",
    "\n",
    "        if output.numpy()[0][0] == 3 or len(results) >= max_length:\n",
    "            end = True\n",
    "        decoder_input = output\n",
    "\n",
    "    results =[en_vectorizer.get_vocabulary()[i] for i in results]\n",
    "    return \" \".join(results)\n",
    "\n",
    "print(translate(\"Ve ben Ã¶lÃ¼m oldum\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-11T18:29:39.255600500Z"
    }
   },
   "id": "4fdb4b9c5fb23ca6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"student\" in en_vectorizer.get_vocabulary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-11T18:29:39.256599400Z"
    }
   },
   "id": "e877b41dcf637c40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# IDEA: Train word2vec on both languages, then use the word2vec embeddings as input to the encoder and decoder instead of random embeddings.\n",
    "# IDEA: Use better tokenization method such as BPE."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-11T18:29:39.257599900Z"
    }
   },
   "id": "e62223844c3738b4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
