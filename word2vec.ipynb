{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "haJUNjSB60Kh"
   },
   "source": [
    "# word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mk4-Hpe1CH16"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T11:08:04.396269Z",
     "iopub.status.busy": "2023-07-28T11:08:04.395997Z",
     "iopub.status.idle": "2023-07-28T11:08:06.753162Z",
     "shell.execute_reply": "2023-07-28T11:08:06.752057Z"
    },
    "id": "RutaI-Tpev3T",
    "ExecuteTime": {
     "end_time": "2023-09-07T20:12:34.602617300Z",
     "start_time": "2023-09-07T20:12:31.461921Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-07 23:12:32.145306: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-07 23:12:33.027608: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import re\n",
    "import string\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T11:08:06.757829Z",
     "iopub.status.busy": "2023-07-28T11:08:06.757354Z",
     "iopub.status.idle": "2023-07-28T11:08:06.767604Z",
     "shell.execute_reply": "2023-07-28T11:08:06.766621Z"
    },
    "id": "10pyUMFkGKVQ",
    "ExecuteTime": {
     "end_time": "2023-09-07T20:12:34.612633700Z",
     "start_time": "2023-09-07T20:12:34.604130Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T11:08:06.771378Z",
     "iopub.status.busy": "2023-07-28T11:08:06.771077Z",
     "iopub.status.idle": "2023-07-28T11:08:06.775396Z",
     "shell.execute_reply": "2023-07-28T11:08:06.774355Z"
    },
    "id": "XkJ5299Tek6B",
    "ExecuteTime": {
     "end_time": "2023-09-07T20:12:34.621156600Z",
     "start_time": "2023-09-07T20:12:34.610128400Z"
    }
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aj--8RFK6fgW"
   },
   "source": [
    "### Generate training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dy5hl4lQ0B2M"
   },
   "source": [
    "Compile all the steps described above into a function that can be called on a list of vectorized sentences obtained from any text dataset. Notice that the sampling table is built before sampling skip-gram word pairs. You will use this function in the later sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T11:08:10.540373Z",
     "iopub.status.busy": "2023-07-28T11:08:10.540161Z",
     "iopub.status.idle": "2023-07-28T11:08:10.546597Z",
     "shell.execute_reply": "2023-07-28T11:08:10.545955Z"
    },
    "id": "63INISDEX1Hu",
    "ExecuteTime": {
     "end_time": "2023-09-07T20:12:34.657337400Z",
     "start_time": "2023-09-07T20:12:34.616160700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generates skip-gram pairs with negative sampling for a list of sequences\n",
    "# (int-encoded sentences) based on window size, number of negative samples\n",
    "# and vocabulary size.\n",
    "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
    "  # Elements of each training example are appended to these lists.\n",
    "  targets, contexts, train_labels = [], [], []\n",
    "\n",
    "  # Build the sampling table for `vocab_size` tokens.\n",
    "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "\n",
    "  # Iterate over all sequences (sentences) in the dataset.\n",
    "  for sequence in tqdm.tqdm(sequences):\n",
    "\n",
    "    # Generate positive skip-gram pairs for a sequence (sentence).\n",
    "    skip_grams, labels = tf.keras.preprocessing.sequence.skipgrams(\n",
    "          sequence,\n",
    "          vocabulary_size=vocab_size,\n",
    "          sampling_table=sampling_table,\n",
    "          window_size=window_size,\n",
    "          negative_samples=0.7)\n",
    "    \n",
    "    # Iterate over each positive skip-gram pair to produce training examples\n",
    "    # with a positive context word and negative samples.\n",
    "    for (target_word, context_word), label in zip(skip_grams,labels):\n",
    "\n",
    "      # Append each element from the training example to global lists.\n",
    "      targets.append(target_word)\n",
    "      contexts.append(context_word)\n",
    "      train_labels.append(label)\n",
    "\n",
    "  return targets, contexts, train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "shvPC8Ji2cMK"
   },
   "source": [
    "## Prepare training data for word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j5mbZsZu6uKg"
   },
   "source": [
    "With an understanding of how to work with one sentence for a skip-gram negative sampling based word2vec model, you can proceed to generate training examples from a larger list of sentences!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFlikI6L26nh"
   },
   "source": [
    "### Download text corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rEFavOgN98al"
   },
   "source": [
    "You will use a text file of Shakespeare's writing for this tutorial. Change the following line to run this code on your own data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTNZYqUs5C2V"
   },
   "source": [
    "Use the non empty lines to construct a `tf.data.TextLineDataset` object for the next steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T11:08:10.692071Z",
     "iopub.status.busy": "2023-07-28T11:08:10.691539Z",
     "iopub.status.idle": "2023-07-28T11:08:10.835087Z",
     "shell.execute_reply": "2023-07-28T11:08:10.834440Z"
    },
    "id": "ViDrwy-HjAs9",
    "ExecuteTime": {
     "end_time": "2023-09-07T20:12:45.738952500Z",
     "start_time": "2023-09-07T20:12:45.721249900Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-07 23:12:44.558652: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-07 23:12:44.700126: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-07 23:12:44.700176: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-07 23:12:44.701965: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-07 23:12:44.702002: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-07 23:12:44.702028: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-07 23:12:45.618016: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-07 23:12:45.618386: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-07 23:12:45.618396: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1726] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-09-07 23:12:45.618431: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-07 23:12:45.618904: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5385 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Ti, pci bus id: 0000:2d:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "BASE = \"/mnt/c/Users/mnusr/PycharmProjects/news_crawler/mlm_train_dataset\"\n",
    "text_ds = tf.data.TextLineDataset([f\"{BASE}/train_sentences_5.txt\",f\"{BASE}/train_sentences_4.txt\",f\"{BASE}/train_sentences_3.txt\",f\"{BASE}/train_sentences_2.txt\",f\"{BASE}/train_sentences_1.txt\"])#.filter(lambda x: tf.cast(tf.strings.length(x), bool))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfsc88zE9upk"
   },
   "source": [
    "### Vectorize sentences from the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XfgZo8zR94KK"
   },
   "source": [
    "You can use the `TextVectorization` layer to vectorize sentences from the corpus. Learn more about using this layer in this [Text classification](https://www.tensorflow.org/tutorials/keras/text_classification) tutorial. Notice from the first few sentences above that the text needs to be in one case and punctuation needs to be removed. To do this, define a `custom_standardization function` that can be used in the TextVectorization layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T11:08:10.839001Z",
     "iopub.status.busy": "2023-07-28T11:08:10.838735Z",
     "iopub.status.idle": "2023-07-28T11:08:10.854250Z",
     "shell.execute_reply": "2023-07-28T11:08:10.853659Z"
    },
    "id": "2MlsXzo-ZlfK",
    "ExecuteTime": {
     "end_time": "2023-09-07T20:12:47.782495900Z",
     "start_time": "2023-09-07T20:12:47.766639Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now, create a custom standardization function to lowercase the text and\n",
    "# remove punctuation.\n",
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  return tf.strings.regex_replace(lowercase,\n",
    "                                  '[%s]' % re.escape(string.punctuation), '')\n",
    "\n",
    "\n",
    "# Define the vocabulary size and the number of words in a sequence.\n",
    "vocab_size = 10000\n",
    "sequence_length = 15\n",
    "\n",
    "# Use the `TextVectorization` layer to normalize, split, and map strings to\n",
    "# integers. Set the `output_sequence_length` length to pad all samples to the\n",
    "# same length.\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g92LuvnyBmz1"
   },
   "source": [
    "Call `TextVectorization.adapt` on the text dataset to create vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T11:08:10.857314Z",
     "iopub.status.busy": "2023-07-28T11:08:10.857086Z",
     "iopub.status.idle": "2023-07-28T11:08:12.087865Z",
     "shell.execute_reply": "2023-07-28T11:08:12.087059Z"
    },
    "id": "seZau_iYMPFT",
    "ExecuteTime": {
     "end_time": "2023-09-07T20:13:52.103479900Z",
     "start_time": "2023-09-07T20:12:49.030793200Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-07 23:13:47.686656: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 18216012323532874981\n",
      "2023-09-07 23:13:47.686714: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 13521184409493339783\n",
      "2023-09-07 23:13:47.686735: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 866342992198037303\n"
     ]
    }
   ],
   "source": [
    "vectorize_layer.adapt(text_ds.batch(4096*8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jg2z7eeHMnH-"
   },
   "source": [
    "Once the state of the layer has been adapted to represent the text corpus, the vocabulary can be accessed with `TextVectorization.get_vocabulary`. This function returns a list of all vocabulary tokens sorted (descending) by their frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T11:08:12.092193Z",
     "iopub.status.busy": "2023-07-28T11:08:12.091578Z",
     "iopub.status.idle": "2023-07-28T11:08:12.102367Z",
     "shell.execute_reply": "2023-07-28T11:08:12.101770Z"
    },
    "id": "jgw9pTA7MRaU",
    "ExecuteTime": {
     "end_time": "2023-09-07T20:13:52.115538400Z",
     "start_time": "2023-09-07T20:13:52.105480400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 've', 'bir', 'bu', 'da', 'için', 'de', 'ile', 'olarak', 'çok', 'daha', 'olan', 'ise', 'en', 'sonra', 'kadar', 'göre', 'gibi', 'her']\n"
     ]
    }
   ],
   "source": [
    "# Save the created vocabulary for reference.\n",
    "inverse_vocab = vectorize_layer.get_vocabulary()\n",
    "print(inverse_vocab[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DOQ30Tx6KA2G"
   },
   "source": [
    "The `vectorize_layer` can now be used to generate vectors for each element in the `text_ds` (a `tf.data.Dataset`). Apply `Dataset.batch`, `Dataset.prefetch`, `Dataset.map`, and `Dataset.unbatch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T11:08:12.105508Z",
     "iopub.status.busy": "2023-07-28T11:08:12.105144Z",
     "iopub.status.idle": "2023-07-28T11:08:12.146664Z",
     "shell.execute_reply": "2023-07-28T11:08:12.145982Z"
    },
    "id": "yUVYrDp0araQ",
    "ExecuteTime": {
     "end_time": "2023-09-07T20:13:52.154308300Z",
     "start_time": "2023-09-07T20:13:52.117047100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Vectorize the data in text_ds.\n",
    "text_vector_ds = text_ds.batch(4096*8).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7YyH_SYzB72p"
   },
   "source": [
    "### Obtain sequences from the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFUQLX0_KaRC"
   },
   "source": [
    "You now have a `tf.data.Dataset` of integer encoded sentences. To prepare the dataset for training a word2vec model, flatten the dataset into a list of sentence vector sequences. This step is required as you would iterate over each sentence in the dataset to produce positive and negative examples.\n",
    "\n",
    "Note: Since the `generate_training_data()` defined earlier uses non-TensorFlow Python/NumPy functions, you could also use a `tf.py_function` or `tf.numpy_function` with `tf.data.Dataset.map`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18429092\n"
     ]
    }
   ],
   "source": [
    "sequences = list(text_vector_ds.as_numpy_iterator())\n",
    "print(len(sequences))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T20:42:25.447690500Z",
     "start_time": "2023-09-07T20:13:52.143275700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Inspect a few examples from `sequences`:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T11:08:16.336201Z",
     "iopub.status.busy": "2023-07-28T11:08:16.335630Z",
     "iopub.status.idle": "2023-07-28T11:08:16.340222Z",
     "shell.execute_reply": "2023-07-28T11:08:16.339581Z"
    },
    "id": "WZf1RIbB2Dfb",
    "ExecuteTime": {
     "end_time": "2023-09-07T20:42:25.447690500Z",
     "start_time": "2023-09-07T20:42:25.440051600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3914    2 1299 3674  916 5926    2 2454 9816 4899  311  124 1517  771\n",
      " 3152] => ['radyo', 've', 'televizyon', 'Üst', 'kurumu', 'rtÜk', 've', 'akp’nin', 'sansür', 'uyguladığı', 'iddia', 'edilen', 'dijital', 'yayın', 'platformu']\n",
      "[   1    1    1   35   97  108  729    1    9 2376    1    2    1    1\n",
      " 6459] => ['[UNK]', '[UNK]', '[UNK]', 'yapılan', 'açıklamada', 'şöyle', 'denildi', '[UNK]', 'olarak', 'türkiyedeki', '[UNK]', 've', '[UNK]', '[UNK]', 'derinden']\n",
      "[  64    1 3430 7681    1 2819 6972    0    0    0    0    0    0    0\n",
      "    0] => ['birlikte', '[UNK]', 'birbirinden', 'yetenekli', '[UNK]', 'gurur', 'duyuyoruz', '', '', '', '', '', '', '', '']\n",
      "[ 524  235 3341 3373   12    2 3955    1    1    1    6   10    1    2\n",
      "    4] => ['Şu', 'anda', 'yapım', 'aşamasında', 'olan', 've', 'yakında', '[UNK]', '[UNK]', '[UNK]', 'için', 'çok', '[UNK]', 've', 'bu']\n",
      "[   1   11 1593 2012    8   78    1    1 2994   11  111 9977    1    1\n",
      " 2434] => ['[UNK]', 'daha', 'derin', 'işbirliği', 'ile', 'türk', '[UNK]', '[UNK]', 'dönük', 'daha', 'yüksek', 'hassasiyet', '[UNK]', '[UNK]', 'açıklamasını']\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences[:5]:\n",
    "  print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDzSOjNwCWNh"
   },
   "source": [
    "### Generate training examples from sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BehvYr-nEKyY"
   },
   "source": [
    "`sequences` is now a list of int encoded sentences. Just call the `generate_training_data` function defined earlier to generate training examples for the word2vec model. To recap, the function iterates over each word from each sequence to collect positive and negative context words. Length of target, contexts and labels should be the same, representing the total number of training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T11:08:16.343315Z",
     "iopub.status.busy": "2023-07-28T11:08:16.343060Z",
     "iopub.status.idle": "2023-07-28T11:09:02.403710Z",
     "shell.execute_reply": "2023-07-28T11:09:02.402881Z"
    },
    "id": "44DJ22M6nX5o",
    "ExecuteTime": {
     "end_time": "2023-09-07T20:49:50.384539500Z",
     "start_time": "2023-09-07T20:42:25.443053Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18429092/18429092 [06:55<00:00, 44405.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "targets.shape: (259354210,)\n",
      "contexts.shape: (259354210,)\n",
      "labels.shape: (259354210,)\n"
     ]
    }
   ],
   "source": [
    "targets, contexts, labels = generate_training_data(\n",
    "    sequences=sequences,\n",
    "    window_size=2,\n",
    "    num_ns=4,\n",
    "    vocab_size=vocab_size,\n",
    "    seed=SEED)\n",
    "\n",
    "targets = np.array(targets)\n",
    "contexts = np.array(contexts)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print('\\n')\n",
    "print(f\"targets.shape: {targets.shape}\")\n",
    "print(f\"contexts.shape: {contexts.shape}\")\n",
    "print(f\"labels.shape: {labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sequences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[33], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m \u001B[43msequences\u001B[49m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'sequences' is not defined"
     ]
    }
   ],
   "source": [
    "del sequences"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T21:02:37.427221500Z",
     "start_time": "2023-09-07T21:02:37.417711500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "958"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T21:04:57.755377300Z",
     "start_time": "2023-09-07T21:04:57.647837100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97PqsusOFEpc"
   },
   "source": [
    "### Configure the dataset for performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7jnFVySViQTj"
   },
   "source": [
    "To perform efficient batching for the potentially large number of training examples, use the `tf.data.Dataset` API. After this step, you would have a `tf.data.Dataset` object of `(target_word, context_word), (label)` elements to train your word2vec model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T11:09:02.407497Z",
     "iopub.status.busy": "2023-07-28T11:09:02.407233Z",
     "iopub.status.idle": "2023-07-28T11:09:02.424122Z",
     "shell.execute_reply": "2023-07-28T11:09:02.423384Z"
    },
    "id": "nbu8PxPSnVY2",
    "ExecuteTime": {
     "end_time": "2023-09-07T21:05:06.891926400Z",
     "start_time": "2023-09-07T21:05:06.221178600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_BatchDataset element_spec=((TensorSpec(shape=(1024,), dtype=tf.int64, name=None), TensorSpec(shape=(1024,), dtype=tf.int64, name=None)), TensorSpec(shape=(1024,), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1024\n",
    "BUFFER_SIZE = 5000\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((targets[:50000000], contexts[:50000000]), labels[:50000000]))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "#del dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T21:05:14.735718100Z",
     "start_time": "2023-09-07T21:05:14.715442700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tyrNX6Fs6K3F"
   },
   "source": [
    "Apply `Dataset.cache` and `Dataset.prefetch` to improve performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T11:09:02.427177Z",
     "iopub.status.busy": "2023-07-28T11:09:02.426941Z",
     "iopub.status.idle": "2023-07-28T11:09:02.434351Z",
     "shell.execute_reply": "2023-07-28T11:09:02.433719Z"
    },
    "id": "Y5Ueg6bcFPVL",
    "ExecuteTime": {
     "end_time": "2023-09-07T21:05:15.574426800Z",
     "start_time": "2023-09-07T21:05:15.513921300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_PrefetchDataset element_spec=((TensorSpec(shape=(1024,), dtype=tf.int64, name=None), TensorSpec(shape=(1024,), dtype=tf.int64, name=None)), TensorSpec(shape=(1024,), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((<tf.Tensor: shape=(1024,), dtype=int64, numpy=array([6467, 7348, 8811, ..., 8422, 4164, 1132])>, <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([   1, 4399, 4348, ...,    6,    1,   27])>), <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([1, 0, 1, ..., 1, 1, 1])>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-08 00:05:16.340078: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for x in dataset.take(1):\n",
    "    print(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T21:05:16.343456700Z",
     "start_time": "2023-09-07T21:05:16.218781500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1S-CmUMszyEf"
   },
   "source": [
    "## Model and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQFqaBMPwBqC"
   },
   "source": [
    "The word2vec model can be implemented as a classifier to distinguish between true context words from skip-grams and false context words obtained through negative sampling. You can perform a dot product multiplication between the embeddings of target and context words to obtain predictions for labels and compute the loss function against true labels in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oc7kTbiwD9sy"
   },
   "source": [
    "### Subclassed word2vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jvr9pM1G1sQN"
   },
   "source": [
    "Use the [Keras Subclassing API](https://www.tensorflow.org/guide/keras/custom_layers_and_models) to define your word2vec model with the following layers:\n",
    "\n",
    "* `target_embedding`: A `tf.keras.layers.Embedding` layer, which looks up the embedding of a word when it appears as a target word. The number of parameters in this layer are `(vocab_size * embedding_dim)`.\n",
    "* `context_embedding`: Another `tf.keras.layers.Embedding` layer, which looks up the embedding of a word when it appears as a context word. The number of parameters in this layer are the same as those in `target_embedding`, i.e. `(vocab_size * embedding_dim)`.\n",
    "* `dots`: A `tf.keras.layers.Dot` layer that computes the dot product of target and context embeddings from a training pair.\n",
    "* `flatten`: A `tf.keras.layers.Flatten` layer to flatten the results of `dots` layer into logits.\n",
    "\n",
    "With the subclassed model, you can define the `call()` function that accepts `(target, context)` pairs which can then be passed into their corresponding embedding layer. Reshape the `context_embedding` to perform a dot product with `target_embedding` and return the flattened result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KiAwuIqqw7-7"
   },
   "source": [
    "Key point: The `target_embedding` and `context_embedding` layers can be shared as well. You could also use a concatenation of both embeddings as the final word2vec embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T11:09:02.437574Z",
     "iopub.status.busy": "2023-07-28T11:09:02.437361Z",
     "iopub.status.idle": "2023-07-28T11:09:02.442674Z",
     "shell.execute_reply": "2023-07-28T11:09:02.442081Z"
    },
    "id": "i9ec-sS6xd8Z",
    "ExecuteTime": {
     "end_time": "2023-09-07T21:05:20.854799600Z",
     "start_time": "2023-09-07T21:05:20.803179700Z"
    }
   },
   "outputs": [],
   "source": [
    "class Word2Vec(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim):\n",
    "    super(Word2Vec, self).__init__()\n",
    "    self.target_embedding = layers.Embedding(vocab_size,\n",
    "                                      embedding_dim,\n",
    "                                      input_length=1,\n",
    "                                      name=\"w2v_embedding\")\n",
    "    self.context_embedding = layers.Embedding(vocab_size,\n",
    "                                       embedding_dim,\n",
    "                                       input_length=1)\n",
    "      \n",
    "    self.dot = tf.keras.layers.Dot(axes=-1)\n",
    "\n",
    "  def call(self, pair):\n",
    "    target, context = pair\n",
    "    # target: (batch, dummy?)  # The dummy axis doesn't exist in TF2.7+\n",
    "    # context: (batch, context)\n",
    "    if len(target.shape) == 2:\n",
    "      target = tf.squeeze(target, axis=1)\n",
    "    # target: (batch,)\n",
    "    word_emb = self.target_embedding(target)\n",
    "    # word_emb: (batch, embed)\n",
    "    context_emb = self.context_embedding(context)\n",
    "    # context_emb: (batch, context, embed)\n",
    "    dots = self.dot([word_emb,context_emb])\n",
    "    # dots: (batch, context)\n",
    "    return dots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-RLKz9LFECXu"
   },
   "source": [
    "### Define loss function and compile model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I3Md-9QanqBM"
   },
   "source": [
    "For simplicity, you can use `tf.keras.losses.CategoricalCrossEntropy` as an alternative to the negative sampling loss. If you would like to write your own custom loss function, you can also do so as follows:\n",
    "\n",
    "``` python\n",
    "def custom_loss(x_logit, y_true):\n",
    "      return tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=y_true)\n",
    "```\n",
    "\n",
    "It's time to build your model! Instantiate your word2vec class with an embedding dimension of 128 (you could experiment with different values). Compile the model with the `tf.keras.optimizers.Adam` optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T11:09:02.445669Z",
     "iopub.status.busy": "2023-07-28T11:09:02.445466Z",
     "iopub.status.idle": "2023-07-28T11:09:02.465259Z",
     "shell.execute_reply": "2023-07-28T11:09:02.464684Z"
    },
    "id": "ekQg_KbWnnmQ",
    "ExecuteTime": {
     "end_time": "2023-09-07T21:05:21.999236200Z",
     "start_time": "2023-09-07T21:05:21.983191Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
    "word2vec.compile(optimizer='adam',\n",
    "                 loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                 run_eagerly=False,\n",
    "                 jit_compile=True,\n",
    "                 metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3MUMrluqNX2"
   },
   "source": [
    "Also define a callback to log training statistics for TensorBoard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T11:09:02.468427Z",
     "iopub.status.busy": "2023-07-28T11:09:02.468210Z",
     "iopub.status.idle": "2023-07-28T11:09:02.472148Z",
     "shell.execute_reply": "2023-07-28T11:09:02.471504Z"
    },
    "id": "9d-ftBCeEZIR",
    "ExecuteTime": {
     "end_time": "2023-09-07T21:05:23.402539900Z",
     "start_time": "2023-09-07T21:05:23.385486900Z"
    }
   },
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5wEBotlGZ7B"
   },
   "source": [
    "Train the model on the `dataset` for some number of epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T11:09:02.475254Z",
     "iopub.status.busy": "2023-07-28T11:09:02.475029Z",
     "iopub.status.idle": "2023-07-28T11:09:21.188906Z",
     "shell.execute_reply": "2023-07-28T11:09:21.188254Z"
    },
    "id": "gmC1BJalEZIY",
    "ExecuteTime": {
     "end_time": "2023-09-07T22:40:27.152567100Z",
     "start_time": "2023-09-07T21:05:24.556815800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-08 00:05:25.219440: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f48180aa1a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-09-08 00:05:25.219474: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3070 Ti, Compute Capability 8.6\n",
      "2023-09-08 00:05:25.294425: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-09-08 00:05:26.969064: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1/48828 [..............................] - ETA: 46:47:57 - loss: 0.6933 - accuracy: 0.3799"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-08 00:05:28.018376: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48828/48828 [==============================] - 2644s 54ms/step - loss: 0.3230 - accuracy: 0.8478\n",
      "Epoch 2/2\n",
      "48828/48828 [==============================] - 3058s 63ms/step - loss: 0.2818 - accuracy: 0.8735\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.src.callbacks.History at 0x7f490c2e9b90>"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.fit(dataset, epochs=2, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wze38jG57XvZ"
   },
   "source": [
    "TensorBoard now shows the word2vec model's accuracy and loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "22E9eqS55rgz"
   },
   "outputs": [],
   "source": [
    "#docs_infra: no_execute\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "awF3iRQCZOLj"
   },
   "source": [
    "<!-- <img class=\"tfo-display-only-on-site\" src=\"images/word2vec_tensorboard.png\"/> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TaDW2tIIz8fL"
   },
   "source": [
    "## Embedding lookup and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zp5rv01WG2YA"
   },
   "source": [
    "Obtain the weights from the model using `Model.get_layer` and `Layer.get_weights`. The `TextVectorization.get_vocabulary` function provides the vocabulary to build a metadata file with one token per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T11:09:21.192606Z",
     "iopub.status.busy": "2023-07-28T11:09:21.192346Z",
     "iopub.status.idle": "2023-07-28T11:09:21.204238Z",
     "shell.execute_reply": "2023-07-28T11:09:21.203589Z"
    },
    "id": "_Uamp1YH8RzU",
    "ExecuteTime": {
     "end_time": "2023-09-07T22:40:27.153567400Z",
     "start_time": "2023-09-07T22:40:27.103498500Z"
    }
   },
   "outputs": [],
   "source": [
    "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gWzdmUzS8Sl4"
   },
   "source": [
    "Create and save the vectors and metadata files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T11:09:21.207489Z",
     "iopub.status.busy": "2023-07-28T11:09:21.207248Z",
     "iopub.status.idle": "2023-07-28T11:09:21.523165Z",
     "shell.execute_reply": "2023-07-28T11:09:21.522357Z"
    },
    "id": "VLIahl9s53XT",
    "ExecuteTime": {
     "end_time": "2023-09-07T22:40:27.690394900Z",
     "start_time": "2023-09-07T22:40:27.118824600Z"
    }
   },
   "outputs": [],
   "source": [
    "out_v = io.open('vectors_tr_2.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata_tr_2.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "  if index == 0:\n",
    "    continue  # skip 0, it's padding.\n",
    "  vec = weights[index]\n",
    "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "  out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "word2vec.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
